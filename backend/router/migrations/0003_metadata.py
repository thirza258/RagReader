# Generated by Django 6.0 on 2026-01-18 16:55

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('router', '0002_document_source_path_alter_document_source_type'),
    ]

    operations = [
        migrations.CreateModel(
            name='Metadata',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('llm_model', models.CharField(default='gpt-4o', help_text='LLM for answer generation', max_length=100)),
                ('temperature', models.FloatField(default=0.0, help_text='LLM temperature (0.0 = deterministic)')),
                ('embedding_model', models.CharField(default='text-embedding-3-small', help_text='OpenAI embedding model', max_length=100)),
                ('top_k', models.IntegerField(default=5, help_text='Number of documents to retrieve')),
                ('chunk_strategy', models.CharField(default='paragraph', help_text='Chunking strategy, e.g., "fixed", "paragraph", "semantic"', max_length=32)),
                ('chunk_size', models.IntegerField(default=500, help_text='Max characters per chunk')),
                ('overlap', models.IntegerField(default=50, help_text='Overlap for fixed chunking only')),
                ('vector_store_path', models.TextField(default='./vector_stores', help_text='Path to store vector indices')),
                ('max_retries', models.IntegerField(default=3, help_text='Max iterations for iterative search')),
                ('reranker_model', models.CharField(default='cross-encoder/ms-marco-MiniLM-L-6-v2', help_text='Reranker model name', max_length=200)),
                ('reranker_top_k', models.IntegerField(default=3, help_text='Top K for reranker')),
                ('device', models.CharField(default='cuda', help_text="Device to use - 'cuda' or 'cpu'", max_length=20)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
            ],
        ),
    ]
